{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Artificial Neural Networks\n",
    "In this part we will see:\n",
    "<ol>\n",
    "    <li> how to build an ANN </li>\n",
    "    <li> how to predict the outcome of a single observation </li>\n",
    "    <li> how to evaluate the performance of an ANN with k-Fold Cross Validation </li>\n",
    "    <li> how to tackle overfitting with Dropout </li>\n",
    "    <li> how to do some Parameter Tuning on our ANN to improve its performance </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The business problem that we are going to deal with is a classification problem. \n",
    "<br>Based on our information we are going to predict which customer is leaving or staying in the bank.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "# Import Theano\n",
    "import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"RowNumber\", \"CustomerId\" and \"Surname\" are not determinant in our business problem. \n",
    "<br> So we will not consider them.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure   Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2      0.00              1   \n",
       "1          608     Spain  Female   41       1  83807.86              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0          1               1        101348.88  \n",
       "1          0               1        112542.58  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[:,3:13].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "France     5014\n",
       "Germany    2509\n",
       "Spain      2477\n",
       "Name: Geography, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Geography'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Male      5457\n",
       "Female    4543\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
       "       ..., \n",
       "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
       "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
       "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include our dependent variables into our matrix X of features \n",
    "X = dataset.iloc[:, 3:13].values\n",
    "print (X.shape)\n",
    "print (type(X))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector y of dependent variables\n",
    "y = dataset.iloc[:, 13].values\n",
    "print(y.shape)\n",
    "print(type(y))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 0 'Female' ..., 1 1 101348.88]\n",
      " [608 2 'Female' ..., 0 1 112542.58]\n",
      " [502 0 'Female' ..., 1 0 113931.57]\n",
      " ..., \n",
      " [709 0 'Female' ..., 0 1 42085.58]\n",
      " [772 1 'Male' ..., 1 0 92888.52]\n",
      " [792 0 'Female' ..., 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding our first categorical independent variable \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 0 0 ..., 1 1 101348.88]\n",
      " [608 2 0 ..., 0 1 112542.58]\n",
      " [502 0 0 ..., 1 0 113931.57]\n",
      " ..., \n",
      " [709 0 0 ..., 0 1 42085.58]\n",
      " [772 1 1 ..., 1 0 92888.52]\n",
      " [792 0 0 ..., 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding our second categorical independent variable\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_1.fit_transform(X[:, 2])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that from the first categorical variable (Geography), France is 0, Spain is 2 and Germany is 1.\n",
    "<br>From the second categorical variable (Gender), Female is 0 and Male is 1.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[      1.         0.         0.   ...,       1.         1.    101348.88]\n",
      " [      0.         0.         1.   ...,       0.         1.    112542.58]\n",
      " [      1.         0.         0.   ...,       1.         0.    113931.57]\n",
      " ..., \n",
      " [      1.         0.         0.   ...,       0.         1.     42085.58]\n",
      " [      0.         1.         0.   ...,       1.         0.     92888.52]\n",
      " [      1.         0.         0.   ...,       1.         0.     38190.78]]\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy variable\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(categorical_features=[1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 12)\n",
      "(10000, 11)\n"
     ]
    }
   ],
   "source": [
    "# Remove a dummy variable to avoid the dummy variable trap\n",
    "X[0]\n",
    "print(X.shape)\n",
    "X = X[:, 1:]\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "X[0]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have now only two dummy variables for the countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the training set and the test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is about Feature Scaling. Because lots of computation are done so we need to do feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.57   1.743  0.17  ...,  0.643 -1.032  1.106]\n",
      " [ 1.755 -0.574 -2.305 ...,  0.643  0.969 -0.749]\n",
      " [-0.57  -0.574 -1.191 ...,  0.643 -1.032  1.485]\n",
      " ..., \n",
      " [-0.57  -0.574  0.902 ...,  0.643 -1.032  1.412]\n",
      " [-0.57   1.743 -0.624 ...,  0.643  0.969  0.844]\n",
      " [ 1.755 -0.574 -0.284 ...,  0.643 -1.032  0.325]]\n",
      "(8000, 11)\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "np.set_printoptions(precision=3)\n",
    "print(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.755 -0.574 -0.552 ...,  0.643  0.969  1.611]\n",
      " [-0.57  -0.574 -1.315 ...,  0.643 -1.032  0.496]\n",
      " [-0.57   1.743  0.572 ...,  0.643  0.969 -0.425]\n",
      " ..., \n",
      " [-0.57   1.743 -0.748 ...,  0.643 -1.032  0.719]\n",
      " [ 1.755 -0.574 -0.006 ...,  0.643  0.969 -1.545]\n",
      " [ 1.755 -0.574 -0.799 ...,  0.643 -1.032  1.613]]\n",
      "(2000, 11)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "print(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make the ANN and importing the Keras library and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Stochastic Gradient Descent method we will apply the rectifier function for the hidden layers and the sigmoid function for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\", input_dim=11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding our second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the final layer (the output layer)\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 0.4770 - acc: 0.7959\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4282 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 0.4222 - acc: 0.7994\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.4184 - acc: 0.8241\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.4156 - acc: 0.8292\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.4140 - acc: 0.8296\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.4126 - acc: 0.8309\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.4113 - acc: 0.8324\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.4105 - acc: 0.8346\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 0.4095 - acc: 0.8334\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 0.4090 - acc: 0.8341\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.4083 - acc: 0.8341\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 0.4077 - acc: 0.8365\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 0.4067 - acc: 0.8350\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4054 - acc: 0.8352\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.4042 - acc: 0.8335\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.4027 - acc: 0.8352\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.4018 - acc: 0.8351\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.4014 - acc: 0.8332\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.4002 - acc: 0.8364\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3999 - acc: 0.8342\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3997 - acc: 0.8339\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3986 - acc: 0.8346\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3983 - acc: 0.8339\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3981 - acc: 0.8332\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.3973 - acc: 0.8337\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3969 - acc: 0.8341\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.3970 - acc: 0.8334\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3968 - acc: 0.8370\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3966 - acc: 0.8336\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3966 - acc: 0.8341\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 0.3962 - acc: 0.8354\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.3955 - acc: 0.8347\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3962 - acc: 0.8329\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.3959 - acc: 0.8350\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.3959 - acc: 0.8354\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3959 - acc: 0.8347\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3955 - acc: 0.8339\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3954 - acc: 0.8351\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3952 - acc: 0.8349\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3955 - acc: 0.8347\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3953 - acc: 0.8354\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3949 - acc: 0.8349\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3950 - acc: 0.8369\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3951 - acc: 0.8362\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3958 - acc: 0.8349\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3946 - acc: 0.8357\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3946 - acc: 0.8370\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 186us/step - loss: 0.3949 - acc: 0.8361\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3944 - acc: 0.8352\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 186us/step - loss: 0.3949 - acc: 0.8355\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 0.3946 - acc: 0.8336\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3943 - acc: 0.8355\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3943 - acc: 0.8357\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.3940 - acc: 0.8347\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3944 - acc: 0.8375\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3943 - acc: 0.8336\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 0.3946 - acc: 0.8366 0s - loss: 0.3949 - a\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 0.3943 - acc: 0.8351\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.3937 - acc: 0.8355\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.3939 - acc: 0.8361\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.3937 - acc: 0.8341\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.3937 - acc: 0.8339\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.3945 - acc: 0.8364\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 0.3940 - acc: 0.8347\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.3941 - acc: 0.8355\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.3938 - acc: 0.8355\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.3929 - acc: 0.8340\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 0.3939 - acc: 0.8346\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.3938 - acc: 0.8342\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 0.3936 - acc: 0.8349\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.3933 - acc: 0.8365\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 0.3938 - acc: 0.8359 0s - loss: 0.3954 - ac\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.3933 - acc: 0.8341\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 0.3936 - acc: 0.8352\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.3926 - acc: 0.8350\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.3933 - acc: 0.8331\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.3932 - acc: 0.8366\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.3936 - acc: 0.8357\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3931 - acc: 0.8365\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.3933 - acc: 0.8346\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.3932 - acc: 0.8365\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.3930 - acc: 0.8366\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.3933 - acc: 0.8357\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 0.3934 - acc: 0.8355\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.3933 - acc: 0.8366\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 0.3926 - acc: 0.8336\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 0.3926 - acc: 0.8364\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.3927 - acc: 0.8364\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.3927 - acc: 0.8347\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.3924 - acc: 0.8349\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.3928 - acc: 0.8347\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 0.3924 - acc: 0.8355\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.3931 - acc: 0.8356\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.3927 - acc: 0.8357\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3929 - acc: 0.8344\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.3929 - acc: 0.8356\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.3929 - acc: 0.8351\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.3929 - acc: 0.8339\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 0.3923 - acc: 0.8372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xaeb4470>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting our ANN to the training set\n",
    "classifier.fit(X_train, y_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.201],\n",
       "       [ 0.298],\n",
       "       [ 0.16 ],\n",
       "       ..., \n",
       "       [ 0.202],\n",
       "       [ 0.111],\n",
       "       [ 0.154]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the test results \n",
    "y_pred = classifier.predict(X_test)\n",
    "np.set_printoptions(precision=3)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the probabilities that the customer, in respect with the index, leave the bank.\n",
    "<br> For example the customer with index 0 has 20% to leave the bank.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ..., \n",
       "       [False],\n",
       "       [False],\n",
       "       [False]], dtype=bool)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now using a threshold, not probabilities\n",
    "y_pred = (y_pred > 0.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means if y_pred > 0.5 it returns \"True\" if not it returns \"False\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a prediction for single new observation\n",
    "<br>Predict if the customer with the following informations will leave the bank or not:</br>\n",
    "<ul>\n",
    "    <li>Geography: France</li>\n",
    "    <li>Credit Score: 600</li>\n",
    "    <li>Gender: Male</li>\n",
    "    <li>Age: 40</li>\n",
    "    <li>Tenure: 3</li>\n",
    "    <li>Balance: 60000</li>\n",
    "    <li>Number of Products: 2</li>\n",
    "    <li>Has a credit Card: Yes</li>\n",
    "    <li>Is a Active Member: Yes</li>\n",
    "    <li>Estimated Salary: 50000</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False]], dtype=bool)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prediction = classifier.predict(sc.transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
    "new_prediction = (new_prediction > 0.5)\n",
    "new_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion this customer will not leave the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1551,   44],\n",
       "       [ 271,  134]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have (1551 + 134) correct predictions and (271 + 44) incorrect predictions.\n",
    "<br> (1551 + 134) / 2000 ( total number of observations in our test set) gives us 84,25% of correct predictions.</br>\n",
    "<br> That is close to the % obtained when fitting our ANN to the training set.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Training our model in the train set and the test several times we usually get different accuracies.\n",
    "<br> This is because of the Bias-Variance Tradeoff.</br>\n",
    "<br> We can fix this by using the KFold-Cross-Validation method.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> For parallel computing, \" n_jobs = -1 \" for multi CPU. </br>\n",
    "<br> n_jobs=1 => 1 CPU core.</br>\n",
    "<br> Currently, multi CPU core processing is not supported with my configuration.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "# The above classifier created is only a local classifier, so we need to create a global classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_classifier, batch_size=10, epochs=100)\n",
    "accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.841,  0.841,  0.831,  0.829,  0.857,  0.862,  0.835,  0.861,\n",
       "        0.841,  0.842])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844249994978\n",
      "0.0114999999932\n"
     ]
    }
   ],
   "source": [
    "# Mean of \"accuracies\"\n",
    "print(accuracies.mean())\n",
    "# Variance of \"accuracies\"\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low variance and 84% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving our ANN\n",
    "# Parameters tuning with GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_classifier)\n",
    "parameters = {'batch_size':[25,32], 'epochs':[100,500], 'optimizer':['adam','rmsprop']}\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=parameters, scoring='accuracy', cv=10)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional Neural Network\n",
    "In this section we will see:\n",
    "<ol>\n",
    "    <li> Convolution Operation </li>\n",
    "    <li> ReLU Layer </li>\n",
    "    <li> Pooling </li> \n",
    "    <li> Flattening </li>\n",
    "    <li> Full Connection </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Convolutional Neural (CNN) Network method to solve an image classification problem where our goal will be to classify an image and to predict which class it belongs.\n",
    "<br> Our dataset consists of 10,000 images of cats and dogs. </br>\n",
    "<br> The splitting was about 80%-20%. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the first hidden layer\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By pooling we reduce the complexity of our model without reducing its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full connection\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compliling the CNN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the 'image augmentation' method. \n",
    "<br> It allows us to create many batches of our images and then for each batch it will apply random transformations on a random selection of our images.</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ImageDataGenerator \n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting our CNN\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch=8000,\n",
    "                         epochs=25,\n",
    "                         validation_data=test_set,\n",
    "                         validation_steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code we obtained an accuracy of 84% on the train set and 75% on the test set.\n",
    "<br> We can reduce that difference between the training set result and the test set result by adding a second convolutional layer. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making new predictions\n",
    "# But before going further let us see which index corresponds to a cat or a dog in our training_set file\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the same method to find out a prediction in the second file of images\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size=(64,64)) # Loading the file of images\n",
    "test_image = image.img_to_array(test_image)    \n",
    "test_image = np.expand_dims(test_image, axis=0) # 'expand' method to correspond to the batch otherwise there will be an error\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0] == 1 :\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recurrent Neural Network\n",
    "In this part we will:\n",
    "<ol>\n",
    "    <li> Start building a simple RNN, our first attempt. </li>\n",
    "    <li> Observe the results to identify possible issues and ways of improvement. </li>\n",
    "    <li> Learn how to to evaluate a RNN model, and more generally a Regression model. </li>\n",
    "    <li> Identify the issues and improvements solutions to make several more robust and relevant RNNs. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to predict the stock price of Google.\n",
    "<br> As it is not possible to predict exactly the future stock price, but it is actually possible to predict some trends.</br>\n",
    "<br> So we are going to predict the upward and downward trends that exist in the Google stock price.</br>\n",
    "<br> We are going to train our model on the open Google stock price from the beginning of 2012 to the end of 2016, and try to predict trends in January 2017. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Date    Open    High     Low   Close      Volume\n",
      "0  1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
      "1  1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
      "2  1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
      "3  1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
      "4  1/9/2012  322.04  322.29  309.46  620.76  11,688,800\n",
      "(1258, 6)\n",
      "[[ 325.25]\n",
      " [ 331.27]\n",
      " [ 329.83]\n",
      " ..., \n",
      " [ 793.7 ]\n",
      " [ 783.33]\n",
      " [ 782.75]]\n",
      "(1258, 1)\n"
     ]
    }
   ],
   "source": [
    "# Importing the training set\n",
    "dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "print(dataset_train.head())\n",
    "print(dataset_train.shape)\n",
    "print(training_set)\n",
    "print(training_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08581368]\n",
      " [ 0.09701243]\n",
      " [ 0.09433366]\n",
      " ..., \n",
      " [ 0.95725128]\n",
      " [ 0.93796041]\n",
      " [ 0.93688146]]\n",
      "(1258, 1)\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "print(training_set_scaled)\n",
    "print(training_set_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08581368  0.09701243  0.09433366 ...,  0.07846566  0.08034452\n",
      "   0.08497656]\n",
      " [ 0.09701243  0.09433366  0.09156187 ...,  0.08034452  0.08497656\n",
      "   0.08627874]\n",
      " [ 0.09433366  0.09156187  0.07984225 ...,  0.08497656  0.08627874\n",
      "   0.08471612]\n",
      " ..., \n",
      " [ 0.92106928  0.92438053  0.93048218 ...,  0.95475854  0.95204256\n",
      "   0.95163331]\n",
      " [ 0.92438053  0.93048218  0.9299055  ...,  0.95204256  0.95163331\n",
      "   0.95725128]\n",
      " [ 0.93048218  0.9299055   0.93113327 ...,  0.95163331  0.95725128\n",
      "   0.93796041]]\n",
      "[ 0.08627874  0.08471612  0.07454052 ...,  0.95725128  0.93796041\n",
      "  0.93688146]\n",
      "(1198, 60)\n",
      "(1198,)\n"
     ]
    }
   ],
   "source": [
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.08581368]\n",
      "  [ 0.09701243]\n",
      "  [ 0.09433366]\n",
      "  ..., \n",
      "  [ 0.07846566]\n",
      "  [ 0.08034452]\n",
      "  [ 0.08497656]]\n",
      "\n",
      " [[ 0.09701243]\n",
      "  [ 0.09433366]\n",
      "  [ 0.09156187]\n",
      "  ..., \n",
      "  [ 0.08034452]\n",
      "  [ 0.08497656]\n",
      "  [ 0.08627874]]\n",
      "\n",
      " [[ 0.09433366]\n",
      "  [ 0.09156187]\n",
      "  [ 0.07984225]\n",
      "  ..., \n",
      "  [ 0.08497656]\n",
      "  [ 0.08627874]\n",
      "  [ 0.08471612]]\n",
      "\n",
      " ..., \n",
      " [[ 0.92106928]\n",
      "  [ 0.92438053]\n",
      "  [ 0.93048218]\n",
      "  ..., \n",
      "  [ 0.95475854]\n",
      "  [ 0.95204256]\n",
      "  [ 0.95163331]]\n",
      "\n",
      " [[ 0.92438053]\n",
      "  [ 0.93048218]\n",
      "  [ 0.9299055 ]\n",
      "  ..., \n",
      "  [ 0.95204256]\n",
      "  [ 0.95163331]\n",
      "  [ 0.95725128]]\n",
      "\n",
      " [[ 0.93048218]\n",
      "  [ 0.9299055 ]\n",
      "  [ 0.93113327]\n",
      "  ..., \n",
      "  [ 0.95163331]\n",
      "  [ 0.95725128]\n",
      "  [ 0.93796041]]]\n",
      "(1198, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the data\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) # getting 3 dimensions\n",
    "print(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the RNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation to avoid overfitting\n",
    "regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1))) # Making a stacked LSTM layer\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the second LSTM layer and some Droptout regularisation\n",
    "regressor.add(LSTM(units=50, return_sequences=True)) \n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the third LSTM layer and some Droptout regularisation\n",
    "regressor.add(LSTM(units=50, return_sequences=True)) \n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the fourth LSTM layer and some Droptout regularisation\n",
    "regressor.add(LSTM(units=50)) # The return_sequences parameter is set to be False as we are not adding any more layer \n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN \n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1198/1198 [==============================] - 13s 11ms/step - loss: 0.0387\n",
      "Epoch 2/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0067\n",
      "Epoch 3/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0053\n",
      "Epoch 4/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0051\n",
      "Epoch 5/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0052\n",
      "Epoch 6/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0048\n",
      "Epoch 7/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0041\n",
      "Epoch 8/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0043\n",
      "Epoch 9/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0044\n",
      "Epoch 10/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0048\n",
      "Epoch 11/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0046\n",
      "Epoch 12/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0037\n",
      "Epoch 13/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0042\n",
      "Epoch 14/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0035\n",
      "Epoch 15/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0036\n",
      "Epoch 16/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0038\n",
      "Epoch 17/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0042\n",
      "Epoch 18/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0036\n",
      "Epoch 19/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0036\n",
      "Epoch 20/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0039\n",
      "Epoch 21/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0041\n",
      "Epoch 22/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0037\n",
      "Epoch 23/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0032\n",
      "Epoch 24/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0031\n",
      "Epoch 25/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0030\n",
      "Epoch 26/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0031\n",
      "Epoch 27/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0031\n",
      "Epoch 28/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0028\n",
      "Epoch 29/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0029\n",
      "Epoch 30/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0030\n",
      "Epoch 31/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0032\n",
      "Epoch 32/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0029\n",
      "Epoch 33/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0026\n",
      "Epoch 34/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0027\n",
      "Epoch 35/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0029\n",
      "Epoch 36/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0028\n",
      "Epoch 37/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0026\n",
      "Epoch 38/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0027\n",
      "Epoch 39/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0026\n",
      "Epoch 40/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0029\n",
      "Epoch 41/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0027\n",
      "Epoch 42/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0027\n",
      "Epoch 43/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0029\n",
      "Epoch 44/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0028\n",
      "Epoch 45/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 0.0028\n",
      "Epoch 46/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0027\n",
      "Epoch 47/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0025\n",
      "Epoch 48/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 0.0025\n",
      "Epoch 49/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 0.0025\n",
      "Epoch 50/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0024\n",
      "Epoch 51/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0026\n",
      "Epoch 52/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0023\n",
      "Epoch 53/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0025\n",
      "Epoch 54/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0023\n",
      "Epoch 55/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0021\n",
      "Epoch 56/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0022\n",
      "Epoch 57/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0022\n",
      "Epoch 58/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0026\n",
      "Epoch 59/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0021\n",
      "Epoch 60/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0022\n",
      "Epoch 61/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0020\n",
      "Epoch 62/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0021\n",
      "Epoch 63/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0021\n",
      "Epoch 64/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0020\n",
      "Epoch 65/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0022\n",
      "Epoch 66/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0019\n",
      "Epoch 67/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 0.0020\n",
      "Epoch 68/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0020\n",
      "Epoch 69/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0018\n",
      "Epoch 70/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 0.0021\n",
      "Epoch 71/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0018\n",
      "Epoch 72/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0018\n",
      "Epoch 73/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0020\n",
      "Epoch 74/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0020\n",
      "Epoch 75/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0018\n",
      "Epoch 76/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0017\n",
      "Epoch 77/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0020\n",
      "Epoch 78/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0019\n",
      "Epoch 79/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0020\n",
      "Epoch 80/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0016\n",
      "Epoch 81/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0018\n",
      "Epoch 82/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0016\n",
      "Epoch 83/100\n",
      "1198/1198 [==============================] - 8s 7ms/step - loss: 0.0015\n",
      "Epoch 84/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0016\n",
      "Epoch 85/100\n",
      "1198/1198 [==============================] - 10s 9ms/step - loss: 0.0016\n",
      "Epoch 86/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0020\n",
      "Epoch 87/100\n",
      "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0018\n",
      "Epoch 88/100\n",
      "1198/1198 [==============================] - 11s 9ms/step - loss: 0.0015\n",
      "Epoch 89/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0017\n",
      "Epoch 90/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0016\n",
      "Epoch 91/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0015\n",
      "Epoch 92/100\n",
      "1198/1198 [==============================] - 9s 8ms/step - loss: 0.0015\n",
      "Epoch 93/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0014\n",
      "Epoch 94/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0015\n",
      "Epoch 95/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0015\n",
      "Epoch 96/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0016\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0014\n",
      "Epoch 98/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0014\n",
      "Epoch 99/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0016\n",
      "Epoch 100/100\n",
      "1198/1198 [==============================] - 9s 7ms/step - loss: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xc788780>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the traning set\n",
    "regressor.fit(X_train, y_train, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss is converging around 0.0015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Date    Open    High     Low   Close     Volume\n",
      "0  1/3/2017  778.81  789.63  775.80  786.14  1,657,300\n",
      "1  1/4/2017  788.36  791.34  783.16  786.90  1,073,000\n",
      "2  1/5/2017  786.08  794.48  785.02  794.02  1,335,200\n",
      "3  1/6/2017  795.26  807.90  792.20  806.15  1,640,200\n",
      "4  1/9/2017  806.40  809.97  802.83  806.65  1,272,400\n",
      "(20, 6)\n",
      "[[ 778.81]\n",
      " [ 788.36]\n",
      " [ 786.08]\n",
      " [ 795.26]\n",
      " [ 806.4 ]\n",
      " [ 807.86]\n",
      " [ 805.  ]\n",
      " [ 807.14]\n",
      " [ 807.48]\n",
      " [ 807.08]\n",
      " [ 805.81]\n",
      " [ 805.12]\n",
      " [ 806.91]\n",
      " [ 807.25]\n",
      " [ 822.3 ]\n",
      " [ 829.62]\n",
      " [ 837.81]\n",
      " [ 834.71]\n",
      " [ 814.66]\n",
      " [ 796.86]]\n",
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "# Getting the real stock price of 2017\n",
    "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "print(dataset_test.head())\n",
    "print(dataset_test.shape)\n",
    "print(real_stock_price)\n",
    "print(real_stock_price.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       325.25\n",
      "1       331.27\n",
      "2       329.83\n",
      "3       328.34\n",
      "4       322.04\n",
      "5       313.70\n",
      "6       310.59\n",
      "7       314.43\n",
      "8       311.96\n",
      "9       314.81\n",
      "10      312.14\n",
      "11      319.30\n",
      "12      294.16\n",
      "13      291.91\n",
      "14      292.07\n",
      "15      287.68\n",
      "16      284.92\n",
      "17      284.32\n",
      "18      287.95\n",
      "19      290.41\n",
      "20      291.38\n",
      "21      291.34\n",
      "22      294.23\n",
      "23      296.39\n",
      "24      302.44\n",
      "25      303.18\n",
      "26      304.87\n",
      "27      302.81\n",
      "28      304.11\n",
      "29      304.63\n",
      "         ...  \n",
      "1248    800.40\n",
      "1249    790.22\n",
      "1250    796.76\n",
      "1251    795.84\n",
      "1252    792.36\n",
      "1253    790.90\n",
      "1254    790.68\n",
      "1255    793.70\n",
      "1256    783.33\n",
      "1257    782.75\n",
      "0       778.81\n",
      "1       788.36\n",
      "2       786.08\n",
      "3       795.26\n",
      "4       806.40\n",
      "5       807.86\n",
      "6       805.00\n",
      "7       807.14\n",
      "8       807.48\n",
      "9       807.08\n",
      "10      805.81\n",
      "11      805.12\n",
      "12      806.91\n",
      "13      807.25\n",
      "14      822.30\n",
      "15      829.62\n",
      "16      837.81\n",
      "17      834.71\n",
      "18      814.66\n",
      "19      796.86\n",
      "Name: Open, Length: 1278, dtype: float64\n",
      "(1278,)\n",
      "[[ 0.9299055 ]\n",
      " [ 0.93113327]\n",
      " [ 0.92750577]\n",
      " [ 0.94415507]\n",
      " [ 0.93876032]\n",
      " [ 0.93403527]\n",
      " [ 0.93483518]\n",
      " [ 0.9313937 ]\n",
      " [ 0.94636878]\n",
      " [ 0.96569685]\n",
      " [ 0.97510976]\n",
      " [ 0.95966962]\n",
      " [ 0.97808617]\n",
      " [ 1.        ]\n",
      " [ 0.98076494]\n",
      " [ 0.97083116]\n",
      " [ 0.98450406]\n",
      " [ 0.96054394]\n",
      " [ 0.9371419 ]\n",
      " [ 0.92841729]\n",
      " [ 0.90804747]\n",
      " [ 0.8771858 ]\n",
      " [ 0.92153434]\n",
      " [ 0.93809063]\n",
      " [ 0.93165414]\n",
      " [ 0.95254483]\n",
      " [ 0.88812412]\n",
      " [ 0.88637547]\n",
      " [ 0.87032145]\n",
      " [ 0.88563137]\n",
      " [ 0.90743359]\n",
      " [ 0.91571173]\n",
      " [ 0.89941588]\n",
      " [ 0.91805566]\n",
      " [ 0.9089404 ]\n",
      " [ 0.9024853 ]\n",
      " [ 0.89456061]\n",
      " [ 0.91600938]\n",
      " [ 0.9132934 ]\n",
      " [ 0.88979835]\n",
      " [ 0.86589404]\n",
      " [ 0.89030062]\n",
      " [ 0.90335962]\n",
      " [ 0.89642086]\n",
      " [ 0.91777662]\n",
      " [ 0.93176576]\n",
      " [ 0.94114145]\n",
      " [ 0.95762334]\n",
      " [ 0.96413424]\n",
      " [ 0.96402262]\n",
      " [ 0.96971501]\n",
      " [ 0.95077759]\n",
      " [ 0.96294367]\n",
      " [ 0.96123223]\n",
      " [ 0.95475854]\n",
      " [ 0.95204256]\n",
      " [ 0.95163331]\n",
      " [ 0.95725128]\n",
      " [ 0.93796041]\n",
      " [ 0.93688146]\n",
      " [ 0.92955205]\n",
      " [ 0.94731751]\n",
      " [ 0.94307612]\n",
      " [ 0.96015329]\n",
      " [ 0.98087655]\n",
      " [ 0.98359253]\n",
      " [ 0.97827219]\n",
      " [ 0.98225314]\n",
      " [ 0.98288563]\n",
      " [ 0.98214153]\n",
      " [ 0.979779  ]\n",
      " [ 0.97849542]\n",
      " [ 0.98182528]\n",
      " [ 0.98245777]\n",
      " [ 1.01045465]\n",
      " [ 1.02407173]\n",
      " [ 1.03930724]\n",
      " [ 1.03354044]\n",
      " [ 0.99624228]\n",
      " [ 0.9631297 ]]\n",
      "(80, 1)\n"
     ]
    }
   ],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis=0) # we concatenate the 2 dataframes by adding their lines\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "print(dataset_total)\n",
    "print(dataset_total.shape)\n",
    "print(inputs)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.9299055 ]\n",
      "  [ 0.93113327]\n",
      "  [ 0.92750577]\n",
      "  ..., \n",
      "  [ 0.95725128]\n",
      "  [ 0.93796041]\n",
      "  [ 0.93688146]]\n",
      "\n",
      " [[ 0.93113327]\n",
      "  [ 0.92750577]\n",
      "  [ 0.94415507]\n",
      "  ..., \n",
      "  [ 0.93796041]\n",
      "  [ 0.93688146]\n",
      "  [ 0.92955205]]\n",
      "\n",
      " [[ 0.92750577]\n",
      "  [ 0.94415507]\n",
      "  [ 0.93876032]\n",
      "  ..., \n",
      "  [ 0.93688146]\n",
      "  [ 0.92955205]\n",
      "  [ 0.94731751]]\n",
      "\n",
      " ..., \n",
      " [[ 0.96054394]\n",
      "  [ 0.9371419 ]\n",
      "  [ 0.92841729]\n",
      "  ..., \n",
      "  [ 1.01045465]\n",
      "  [ 1.02407173]\n",
      "  [ 1.03930724]]\n",
      "\n",
      " [[ 0.9371419 ]\n",
      "  [ 0.92841729]\n",
      "  [ 0.90804747]\n",
      "  ..., \n",
      "  [ 1.02407173]\n",
      "  [ 1.03930724]\n",
      "  [ 1.03354044]]\n",
      "\n",
      " [[ 0.92841729]\n",
      "  [ 0.90804747]\n",
      "  [ 0.8771858 ]\n",
      "  ..., \n",
      "  [ 1.03930724]\n",
      "  [ 1.03354044]\n",
      "  [ 0.99624228]]]\n",
      "(20, 60, 1)\n",
      "[[ 793.93713379]\n",
      " [ 791.24395752]\n",
      " [ 790.61590576]\n",
      " [ 791.48120117]\n",
      " [ 794.46252441]\n",
      " [ 800.12524414]\n",
      " [ 806.24719238]\n",
      " [ 810.03894043]\n",
      " [ 811.51586914]\n",
      " [ 811.56115723]\n",
      " [ 811.01635742]\n",
      " [ 810.29772949]\n",
      " [ 809.70227051]\n",
      " [ 809.81488037]\n",
      " [ 810.56610107]\n",
      " [ 814.42138672]\n",
      " [ 821.04486084]\n",
      " [ 829.03692627]\n",
      " [ 834.84307861]\n",
      " [ 833.47949219]]\n",
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "print(X_test)\n",
    "print(X_test.shape)\n",
    "print(predicted_stock_price)\n",
    "print(predicted_stock_price.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x139b5ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "plt.plot(real_stock_price, color='red', label='Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color='blue', label='Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our predictions could follow some trends, such as some ups and downs.\n",
    "<br>Which means that our model is follwing quite well the directions of the stock price.</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
